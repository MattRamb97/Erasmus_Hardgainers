{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3888feb8db75456eb2b2f85fcaf92d3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4a9f3b28d3fa4a8584b1ac1d5bf01495",
              "IPY_MODEL_3e4a91c9f41c474a9b846d0b52ca85dd",
              "IPY_MODEL_fdbe3b1e37734e22b74ec8609615766f"
            ],
            "layout": "IPY_MODEL_6ebf508b9a2643858a0ac351a64866b6",
            "tabbable": null,
            "tooltip": null
          }
        },
        "4a9f3b28d3fa4a8584b1ac1d5bf01495": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_b7e8bf65d6044d589e9773b823a8f3a1",
            "placeholder": "​",
            "style": "IPY_MODEL_1d3b37c876da4521bf20e8eaf5b631e3",
            "tabbable": null,
            "tooltip": null,
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "3e4a91c9f41c474a9b846d0b52ca85dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_b28b883ae1cb485d8a0d8a1369ecd3dd",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c94fdbd1fc62410ab8c64a5dc7a1e7f7",
            "tabbable": null,
            "tooltip": null,
            "value": 2
          }
        },
        "fdbe3b1e37734e22b74ec8609615766f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_e23205631bea4bcabe337ca94a5a678a",
            "placeholder": "​",
            "style": "IPY_MODEL_97d27f368abf4133971154cb361f91ff",
            "tabbable": null,
            "tooltip": null,
            "value": " 2/2 [01:19&lt;00:00, 36.47s/it]"
          }
        },
        "6ebf508b9a2643858a0ac351a64866b6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b7e8bf65d6044d589e9773b823a8f3a1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1d3b37c876da4521bf20e8eaf5b631e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLStyleModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "background": null,
            "description_width": "",
            "font_size": null,
            "text_color": null
          }
        },
        "b28b883ae1cb485d8a0d8a1369ecd3dd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c94fdbd1fc62410ab8c64a5dc7a1e7f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e23205631bea4bcabe337ca94a5a678a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "97d27f368abf4133971154cb361f91ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLStyleModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "background": null,
            "description_width": "",
            "font_size": null,
            "text_color": null
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MattRamb97/Erasmus_Hardgainers/blob/main/Recipes_model_v2_Ame.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RECIPES MODEL**"
      ],
      "metadata": {
        "id": "QgSEjO0i7e_v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We utilized the URLs provided to access the datasets stored in the folder https://raw.githubusercontent.com/MattRamb97/Erasmus_Hardgainers/main/Datasets/\n",
        "\n",
        "using the pandas library. Subsequently, we identified an issue in the 'ingredients' column of the three datasets, characterized by the presence of the word 'ADVERTISEMENT', which we proceeded to remove. Additionally, we printed information regarding one of the datasets (the same information applies to the others) and their respective shapes."
      ],
      "metadata": {
        "id": "7EArwz7S8MEf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "zORKNY3QbWaP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "326701a9-d9f5-4e12-fdc1-54303804a2af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 39802 entries, rmK12Uau.ntP510KeImX506H6Mr6jTu to 2Q3Zpfgt/PUwn1YABjJ5A9T3ZW8xwVa\n",
            "Data columns (total 4 columns):\n",
            " #   Column        Non-Null Count  Dtype \n",
            "---  ------        --------------  ----- \n",
            " 0   title         39522 non-null  object\n",
            " 1   ingredients   39522 non-null  object\n",
            " 2   instructions  39522 non-null  object\n",
            " 3   picture_link  39522 non-null  object\n",
            "dtypes: object(4)\n",
            "memory usage: 2.5+ MB\n",
            "\n",
            "From AR: (39802, 4)\n",
            "From EPI:  (25323, 4)\n",
            "FROM FN: (60039, 4)\n",
            "0    Title: Slow Cooker Chicken and Dumplings\\nIngr...\n",
            "1    Title: Awesome Slow Cooker Pot Roast\\nIngredie...\n",
            "2    Title: Brown Sugar Meatloaf\\nIngredients: ['1/...\n",
            "Name: all, dtype: object\n"
          ]
        }
      ],
      "source": [
        "url_1 = 'https://raw.githubusercontent.com/MattRamb97/Erasmus_Hardgainers/main/Datasets/recipes_raw_nosource_ar.json'\n",
        "url_2 = 'https://raw.githubusercontent.com/MattRamb97/Erasmus_Hardgainers/main/Datasets/recipes_raw_nosource_epi.json'\n",
        "url_3 = 'https://raw.githubusercontent.com/MattRamb97/Erasmus_Hardgainers/main/Datasets/recipes_raw_nosource_fn.json'\n",
        "\n",
        "import pandas as pd\n",
        "df_ar = pd.read_json(url_1, orient='records', dtype='dict').transpose()\n",
        "df_ar.info()\n",
        "print()\n",
        "df_epi = pd.read_json(url_2, orient='records', dtype='dict').transpose()\n",
        "df_fn = pd.read_json(url_3, orient='records', dtype='dict').transpose()\n",
        "\n",
        "df_ar['title'] = df_ar['title'].astype(str)\n",
        "df_epi['title'] = df_epi['title'].astype(str)\n",
        "df_fn['title'] = df_fn['title'].astype(str)\n",
        "\n",
        "# Remove the word 'ADVERTISEMENT' from each text entry in the 'ingridients'\n",
        "df_ar['ingredients'] = df_ar['ingredients'].astype(str).str.replace('ADVERTISEMENT', '',regex=True)\n",
        "df_epi['ingredients'] = df_epi['ingredients'].astype(str).str.replace('ADVERTISEMENT', '',regex=True)\n",
        "df_fn['ingredients'] = df_fn['ingredients'].astype(str).str.replace('ADVERTISEMENT', '',regex=True)\n",
        "\n",
        "df_ar['instructions'] = df_ar['instructions'].astype(str)\n",
        "df_epi['instructions'] = df_epi['instructions'].astype(str)\n",
        "df_fn['instructions'] = df_fn['instructions'].astype(str)\n",
        "\n",
        "print('From AR:', df_ar.shape)\n",
        "print('From EPI: ', df_epi.shape)\n",
        "print('FROM FN:', df_fn.shape)\n",
        "\n",
        "\n",
        "train_dataset = []\n",
        "for df in [df_ar, df_epi, df_fn]:\n",
        "    for index, row in df.iterrows():\n",
        "        # Aggiungi \"Title:\", \"Ingredients:\" e \"Instructions:\" prima di ogni contenuto\n",
        "        titolo = f\"Title: {row['title']}\\n\"\n",
        "        ingredienti = f\"Ingredients: {row['ingredients']}\\n\"\n",
        "        istruzioni = f\"Instructions: {row['instructions']}\"\n",
        "\n",
        "        # Concatena tutti i contenuti\n",
        "        riga_completa = f\"{titolo}{ingredienti}{istruzioni}\"\n",
        "\n",
        "        # Aggiungi la riga al DataFrame\n",
        "        train_dataset.append(riga_completa)\n",
        "\n",
        "\n",
        "df_all = pd.DataFrame({\"all\": train_dataset})\n",
        "prime_tre_celle = df_all['all'].head(3)\n",
        "\n",
        "print(prime_tre_celle)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We cleaned the three datasets using two function:\n",
        "- uncontract(): expands contracted words commonly found in English sentences into their full forms using regular expressions.\n",
        "- clean_recepies(): removes non-alphanumeric characters from the input text except for spaces and hyphens. It then replaces multiple spaces and hyphens with a single space and normalizes units of measurement (e.g., cups, tablespoons) by removing them. Next, it removes numerical digits, assuming they represent quantities. Finally, it normalizes whitespace by removing extra spaces and ensuring consistent spacing between words."
      ],
      "metadata": {
        "id": "3GcEDJTI-KL4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install accelerate\n",
        "!pip install -i https://pypi.org/simple/ bitsandbytes\n",
        "!pip install -q -U git+https://github.com/huggingface/transformers.git\n",
        "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
        "!pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
        "!pip install -q -U datasets scipy ipywidgets matplotlib"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fGLa2UZXF9g6",
        "outputId": "458a8dc0-b41f-457d-c4ff-9c16f3422d78"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.30.0.dev0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.2.1+cu121)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.22.2)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.13.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Looking in indexes: https://pypi.org/simple/\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (0.43.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.2.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.25.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.13.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->bitsandbytes) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->bitsandbytes) (1.3.0)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a formatting_func to structure training examples as prompts."
      ],
      "metadata": {
        "id": "MhNc7-SuHIUc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def formatting_func(example):\n",
        "    text = f\"### This is your recipe: {example}\"\n",
        "    return text\n",
        "\n",
        "'''\n",
        "def formatting_func(example):\n",
        "    text = f\"### Title: {example['input']}\\n ### Ingredients: {example['output']}\"\n",
        "    return text\n",
        "'''"
      ],
      "metadata": {
        "id": "iP0ZOpuRGbb8",
        "outputId": "81b1a144-96d0-4170-bfee-1369d568c362",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ndef formatting_func(example):\\n    text = f\"### Title: {example[\\'input\\']}\\n ### Ingredients: {example[\\'output\\']}\"\\n    return text\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Mistral - mistralai/Mistral-7B-v0.1"
      ],
      "metadata": {
        "id": "2dXRlDN-HM-U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "base_model_id = \"mistralai/Mistral-7B-v0.1\"\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(base_model_id, quantization_config=bnb_config, device_map=\"auto\")"
      ],
      "metadata": {
        "id": "I_MSkGl3UOiw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 291,
          "referenced_widgets": [
            "3888feb8db75456eb2b2f85fcaf92d3c",
            "4a9f3b28d3fa4a8584b1ac1d5bf01495",
            "3e4a91c9f41c474a9b846d0b52ca85dd",
            "fdbe3b1e37734e22b74ec8609615766f",
            "6ebf508b9a2643858a0ac351a64866b6",
            "b7e8bf65d6044d589e9773b823a8f3a1",
            "1d3b37c876da4521bf20e8eaf5b631e3",
            "b28b883ae1cb485d8a0d8a1369ecd3dd",
            "c94fdbd1fc62410ab8c64a5dc7a1e7f7",
            "e23205631bea4bcabe337ca94a5a678a",
            "97d27f368abf4133971154cb361f91ff"
          ]
        },
        "outputId": "6a68f657-548d-4e14-d972-459616660a47"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    A token is already saved on your machine. Run `huggingface-cli whoami` to get more information or `huggingface-cli logout` if you want to log out.\n",
            "    Setting a new token will erase the existing one.\n",
            "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): \n",
            "Add token as git credential? (Y/n) n\n",
            "Token is valid (permission: write).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3888feb8db75456eb2b2f85fcaf92d3c"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set up the tokenizer. Add padding on the left as it makes training use less memory. For model_max_length, it's helpful to get a distribution of your data lengths. Let's first tokenize without the truncation/padding, so we can get a length distribution."
      ],
      "metadata": {
        "id": "uh_eFIQ4IEYS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    base_model_id,\n",
        "    padding_side=\"left\",\n",
        "    add_eos_token=True,\n",
        "    add_bos_token=True,\n",
        ")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "def generate_and_tokenize_prompt(all):\n",
        "    return tokenizer(formatting_func(all))\n",
        "'''\n",
        "def generate_and_tokenize_prompt(prompt):\n",
        "    return tokenizer(formatting_func(prompt))\n",
        "'''"
      ],
      "metadata": {
        "id": "SrALxLNtILcT",
        "outputId": "2f119cba-bd9f-4ca9-9385-4f40822f42f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ndef generate_and_tokenize_prompt(prompt):\\n    return tokenizer(formatting_func(prompt))\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_all.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FVyEf-1de2Sw",
        "outputId": "dc717ace-7f0c-461b-e94d-8e3ef75719a5"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['all'], dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reformat the prompt and tokenize each sample"
      ],
      "metadata": {
        "id": "6qxSioghIXcp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_df, val_df = train_test_split(df_all, test_size=0.35, random_state=42)\n",
        "print(train_df.columns)\n",
        "print(val_df.columns)\n",
        "\n",
        "tokenized_train_dataset =[]\n",
        "tokenized_val_dataset =[]\n",
        "\n",
        "tokenized_train_dataset.append(generate_and_tokenize_prompt(train_df))\n",
        "tokenized_val_dataset.append(generate_and_tokenize_prompt(val_df))\n",
        "\n",
        "#tokenized_titles = []\n",
        "#tokenized_ingredients = []\n",
        "#tokenized_instructions = []\n",
        "\n",
        "#for title, ingredients, instructions in zip(df_ar['title'].iloc[:200], df_ar['ingredients'].iloc[:200], df_ar['instructions'].iloc[:200]):\n",
        "#    tokenized_titles.append(generate_and_tokenize_prompt(title, ingredients, instructions))\n",
        "#    tokenized_ingredients.append(generate_and_tokenize_prompt(title, ingredients, instructions))\n",
        "#    tokenized_instructions.append(generate_and_tokenize_prompt(title, ingredients, instructions))\n",
        "\n",
        "#tokenized_titles = [generate_and_tokenize_prompt(title, ingredients) for title, ingredients in zip(df_ar['title'], df_ar['ingredients'])]"
      ],
      "metadata": {
        "id": "Efy3OWxtIbNy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "827d4ea2-5d2e-4a39-cc76-771e9be3e9f7"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['all'], dtype='object')\n",
            "Index(['all'], dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's get a distribution of our dataset lengths, so we can determine the appropriate max_length for our input tensors."
      ],
      "metadata": {
        "id": "_1v-yvq2KZDG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_data_lengths(tokenized_train_dataset, tokenized_val_dataset):\n",
        "    lengths = [len(x['input_ids']) for x in tokenized_train_dataset]\n",
        "    lengths += [len(x['input_ids']) for x in tokenized_val_dataset]\n",
        "    print(len(lengths))\n",
        "\n",
        "    # Plotting the histogram\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.hist(lengths, bins=20, alpha=0.7, color='blue')\n",
        "    plt.xlabel('Length of input_ids')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.title('Distribution of Lengths of input_ids')\n",
        "    plt.show()\n",
        "\n",
        "plot_data_lengths(tokenized_train_dataset, tokenized_val_dataset)"
      ],
      "metadata": {
        "id": "sc9oNJ75KZwZ",
        "outputId": "4b269d4c-2178-4ef8-a3c0-755e35dd3056",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 582
        }
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 285 # This was an appropriate max length for my dataset\n",
        "\n",
        "def generate_and_tokenize_prompt2(prompt):\n",
        "    result = tokenizer(\n",
        "        formatting_func(prompt),\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
        "    return result"
      ],
      "metadata": {
        "id": "1pJIRMvzVWuE"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#plot_data_lengths(train_df, val_df)"
      ],
      "metadata": {
        "id": "yvTxyr_H3wqr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "outputId": "22ed20a7-c7ee-4880-a034-12276bf5df16"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "string indices must be integers",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-81-8903eb6a4cb5>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot_data_lengths\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-64-2a8d5bf324ca>\u001b[0m in \u001b[0;36mplot_data_lengths\u001b[0;34m(tokenized_train_dataset, tokenized_val_dataset)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplot_data_lengths\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized_train_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenized_val_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokenized_train_dataset\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mlengths\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokenized_val_dataset\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-64-2a8d5bf324ca>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplot_data_lengths\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized_train_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenized_val_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokenized_train_dataset\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mlengths\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokenized_val_dataset\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: string indices must be integers"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval_prompt = f\"### Recipes {all}\"\n",
        "\n",
        "# Init an eval tokenizer that doesn't add padding or eos token\n",
        "eval_tokenizer = AutoTokenizer.from_pretrained(\n",
        "    base_model_id,\n",
        "    add_bos_token=True,\n",
        ")\n",
        "\n",
        "model_input = eval_tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    print(eval_tokenizer.decode(model.generate(**model_input, max_new_tokens=256, repetition_penalty=1.15)[0], skip_special_tokens=True))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fke7hj1JPyQT",
        "outputId": "b0c2d432-e72b-47d3-eec0-8c05242efc76"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### Recipes <built-in function all>\n",
            "\n",
            "```python\n",
            "#!/usr/bin/env python3\n",
            "import sys\n",
            "sys.path.append('/home/john/.local/lib/python3.6/site-packages')\n",
            "from pyspark import SparkConf, SparkContext\n",
            "conf = (SparkConf()\n",
            "        .setMaster(\"local\")\n",
            "        .setAppName(\"PythonWordCount\"))\n",
            "sc = SparkContext(conf=conf)\n",
            "lines = sc.textFile(\"/home/john/data/wordcount/shakespeare/*.txt\", 2)\n",
            "words = lines.flatMap(lambda line: line.lower().split(\" \"))\n",
            "pairs = words.map(lambda word: (word, 1))\n",
            "by_word = pairs.reduceByKey(lambda a, b: a + b)\n",
            "result = by_word.collectAsMap()\n",
            "for k, v in result.items():\n",
            "    print(\"%s:%d\" % (k, v))\n",
            "```\n",
            "\n",
            "### Recipes <built-in function any>\n",
            "\n",
            "```python\n",
            "#!/usr/bin/env python3\n",
            "import sys\n",
            "sys.path.append('/home\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "k4qY_S0EaOPs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Set Up LoRA**"
      ],
      "metadata": {
        "id": "JMDv9mCeV-il"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, to start our fine-tuning, we have to apply some preprocessing to the model to prepare it for training. For that use the prepare_model_for_kbit_training method from PEFT."
      ],
      "metadata": {
        "id": "txXs94_zWCcQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import prepare_model_for_kbit_training\n",
        "\n",
        "model.gradient_checkpointing_enable()\n",
        "model = prepare_model_for_kbit_training(model)"
      ],
      "metadata": {
        "id": "k-rUo6KTWFMU"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_trainable_parameters(model):\n",
        "    \"\"\"\n",
        "    Prints the number of trainable parameters in the model.\n",
        "    \"\"\"\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    print(\n",
        "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
        "    )"
      ],
      "metadata": {
        "id": "JTws6uDbWIOD"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's print the model to examine its layers, as we will apply QLoRA to all the linear layers of the model. Those layers are q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, down_proj, and lm_head."
      ],
      "metadata": {
        "id": "YMHP0xk-WNVV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "id": "z-6Iqd0KYCID",
        "outputId": "ca2e102e-8f11-4076-a447-d54d734f8a24",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MistralForCausalLM(\n",
            "  (model): MistralModel(\n",
            "    (embed_tokens): Embedding(32000, 4096)\n",
            "    (layers): ModuleList(\n",
            "      (0-31): 32 x MistralDecoderLayer(\n",
            "        (self_attn): MistralSdpaAttention(\n",
            "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
            "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
            "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
            "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
            "          (rotary_emb): MistralRotaryEmbedding()\n",
            "        )\n",
            "        (mlp): MistralMLP(\n",
            "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
            "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
            "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
            "          (act_fn): SiLU()\n",
            "        )\n",
            "        (input_layernorm): MistralRMSNorm()\n",
            "        (post_attention_layernorm): MistralRMSNorm()\n",
            "      )\n",
            "    )\n",
            "    (norm): MistralRMSNorm()\n",
            "  )\n",
            "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we define the LoRA config.\n",
        "\n",
        "r is the rank of the low-rank matrix used in the adapters, which thus controls the number of parameters trained. A higher rank will allow for more expressivity, but there is a compute tradeoff.\n",
        "\n",
        "alpha is the scaling factor for the learned weights. The weight matrix is scaled by alpha/r, and thus a higher value for alpha assigns more weight to the LoRA activations.\n",
        "\n",
        "The values used in the QLoRA paper were r=64 and lora_alpha=16, and these are said to generalize well, but we will use r=32 and lora_alpha=64 so that we have more emphasis on the new fine-tuned data while also reducing computational complexity."
      ],
      "metadata": {
        "id": "d13VfHnQYHw5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "config = LoraConfig(\n",
        "    r=32,\n",
        "    lora_alpha=64,\n",
        "    target_modules=[\n",
        "        \"q_proj\",\n",
        "        \"k_proj\",\n",
        "        \"v_proj\",\n",
        "        \"o_proj\",\n",
        "        \"gate_proj\",\n",
        "        \"up_proj\",\n",
        "        \"down_proj\",\n",
        "        \"lm_head\",\n",
        "    ],\n",
        "    bias=\"none\",\n",
        "    lora_dropout=0.05,  # Conventional\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, config)\n",
        "print_trainable_parameters(model)"
      ],
      "metadata": {
        "id": "yvugWrLvYPZU",
        "outputId": "62c54622-ad53-41bf-8ccc-e3268592a1f5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 85041152 || all params: 3837112320 || trainable%: 2.2162799758751914\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "See how the model looks different now, with the LoRA adapters added:"
      ],
      "metadata": {
        "id": "4xhf8d0LYUdD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "id": "v6y9Rys9YWey",
        "outputId": "9c571acb-1493-4dd1-e655-4f7b68f2b95e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PeftModelForCausalLM(\n",
            "  (base_model): LoraModel(\n",
            "    (model): MistralForCausalLM(\n",
            "      (model): MistralModel(\n",
            "        (embed_tokens): Embedding(32000, 4096)\n",
            "        (layers): ModuleList(\n",
            "          (0-31): 32 x MistralDecoderLayer(\n",
            "            (self_attn): MistralSdpaAttention(\n",
            "              (q_proj): lora.Linear4bit(\n",
            "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "              )\n",
            "              (k_proj): lora.Linear4bit(\n",
            "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "              )\n",
            "              (v_proj): lora.Linear4bit(\n",
            "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "              )\n",
            "              (o_proj): lora.Linear4bit(\n",
            "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "              )\n",
            "              (rotary_emb): MistralRotaryEmbedding()\n",
            "            )\n",
            "            (mlp): MistralMLP(\n",
            "              (gate_proj): lora.Linear4bit(\n",
            "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=32, out_features=14336, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "              )\n",
            "              (up_proj): lora.Linear4bit(\n",
            "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=32, out_features=14336, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "              )\n",
            "              (down_proj): lora.Linear4bit(\n",
            "                (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=14336, out_features=32, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "              )\n",
            "              (act_fn): SiLU()\n",
            "            )\n",
            "            (input_layernorm): MistralRMSNorm()\n",
            "            (post_attention_layernorm): MistralRMSNorm()\n",
            "          )\n",
            "        )\n",
            "        (norm): MistralRMSNorm()\n",
            "      )\n",
            "      (lm_head): lora.Linear(\n",
            "        (base_layer): Linear(in_features=4096, out_features=32000, bias=False)\n",
            "        (lora_dropout): ModuleDict(\n",
            "          (default): Dropout(p=0.05, inplace=False)\n",
            "        )\n",
            "        (lora_A): ModuleDict(\n",
            "          (default): Linear(in_features=4096, out_features=32, bias=False)\n",
            "        )\n",
            "        (lora_B): ModuleDict(\n",
            "          (default): Linear(in_features=32, out_features=32000, bias=False)\n",
            "        )\n",
            "        (lora_embedding_A): ParameterDict()\n",
            "        (lora_embedding_B): ParameterDict()\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "tIu5yNhYaRmq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TRAINING LoRA MODEL**"
      ],
      "metadata": {
        "id": "nkBAVmb3ZZ6e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.device_count() > 1: # If more than 1 GPU\n",
        "    model.is_parallelizable = True\n",
        "    model.model_parallel = True"
      ],
      "metadata": {
        "id": "PGTxRTgLZi_0"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from accelerate import FullyShardedDataParallelPlugin, Accelerator\n",
        "from torch.distributed.fsdp.fully_sharded_data_parallel import FullOptimStateDictConfig, FullStateDictConfig\n",
        "\n",
        "fsdp_plugin = FullyShardedDataParallelPlugin(\n",
        "    state_dict_config=FullStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
        "    optim_state_dict_config=FullOptimStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
        ")\n",
        "\n",
        "accelerator = Accelerator(fsdp_plugin=fsdp_plugin)\n",
        "\n",
        "model = accelerator.prepare_model(model)"
      ],
      "metadata": {
        "id": "J4Nh-UgTZktM"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q wandb -U\n",
        "\n",
        "import wandb, os\n",
        "wandb.login()\n",
        "\n",
        "wandb_project = \"journal-finetune\"\n",
        "if len(wandb_project) > 0:\n",
        "    os.environ[\"WANDB_PROJECT\"] = wandb_project"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "id": "_QY-O7nOa3AZ",
        "outputId": "ea68439c-be27-4544-cedb-ff1aee6efdec"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m266.8/266.8 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "from datetime import datetime\n",
        "\n",
        "'''\n",
        "tokenized_titles2_train = tokenized_titles2[:100]\n",
        "tokenized_titles2_eval = tokenized_titles2[100:200]\n",
        "'''\n",
        "\n",
        "project = \"recepies-model\"\n",
        "base_model_name = \"mistral\"\n",
        "run_name = base_model_name + \"-\" + project\n",
        "output_dir = \"./\" + run_name\n",
        "\n",
        "'''\n",
        "trainer = transformers.Trainer(\n",
        "    model=model,\n",
        "    train_dataset=tokenized_titles2_train,\n",
        "    eval_dataset=tokenized_titles2_eval,\n",
        "    args=transformers.TrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        warmup_steps=1,\n",
        "        per_device_train_batch_size=2,\n",
        "        gradient_accumulation_steps=1,\n",
        "        gradient_checkpointing=True,\n",
        "        max_steps=500,\n",
        "        learning_rate=2.5e-5, # Want a small lr for finetuning\n",
        "        fp16=True,\n",
        "        optim=\"paged_adamw_8bit\",\n",
        "        logging_steps=25,              # When to start reporting loss\n",
        "        logging_dir=\"./logs\",        # Directory for storing logs\n",
        "        save_strategy=\"steps\",       # Save the model checkpoint every logging step\n",
        "        save_steps=25,                # Save checkpoints every 50 steps\n",
        "        evaluation_strategy=\"steps\", # Evaluate the model every logging step\n",
        "        eval_steps=25,               # Evaluate and save checkpoints every 50 steps\n",
        "        do_eval=True,                # Perform evaluation at the end of training\n",
        "        report_to=\"wandb\",           # Comment this out if you don't want to use weights & baises\n",
        "        run_name=f\"{run_name}-{datetime.now().strftime('%Y-%m-%d-%H-%M')}\"          # Name of the W&B run (optional)\n",
        "    ),\n",
        "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
        ")\n",
        "\n",
        "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
        "trainer.train()\n",
        "'''\n",
        "import transformers\n",
        "from transformers import TrainingArguments, Trainer\n",
        "from datetime import datetime\n",
        "import torch\n",
        "\n",
        "# Initialize Trainer with your model and tokenizer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    train_dataset=tokenized_train_dataset,\n",
        "    eval_dataset=tokenized_val_dataset,\n",
        "    args=TrainingArguments(\n",
        "        output_dir='./output',\n",
        "        warmup_steps=1,\n",
        "        per_device_train_batch_size=2,\n",
        "        gradient_accumulation_steps=1,\n",
        "        gradient_checkpointing=True,\n",
        "        max_steps=500,\n",
        "        learning_rate=2.5e-5,\n",
        "        fp16=True,\n",
        "        optim=\"paged_adamw_8bit\",\n",
        "        logging_steps=25,\n",
        "        logging_dir=\"./logs\",\n",
        "        save_strategy=\"steps\",\n",
        "        save_steps=25,\n",
        "        evaluation_strategy=\"steps\",\n",
        "        eval_steps=25,\n",
        "        do_eval=True,\n",
        "        report_to=\"wandb\",\n",
        "        run_name=f\"{run_name}-{datetime.now().strftime('%Y-%m-%d-%H-%M')}\"\n",
        "    ),\n",
        "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
        ")\n",
        "model.config.use_cache = False\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        },
        "id": "BeEHSdGmZyjQ",
        "outputId": "6363227c-634d-4115-f625-3ac585ebaf0e"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "max_steps is given, it will override any value given in num_train_epochs\n",
            "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:180: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
            "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='7' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [  7/500 00:11 < 18:42, 0.44 it/s, Epoch 6/500]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-86-2aec4bf67f5e>\u001b[0m in \u001b[0;36m<cell line: 78>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     76\u001b[0m )\n\u001b[1;32m     77\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_cache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1873\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1874\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1875\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   1876\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1877\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2205\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccumulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2206\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2208\u001b[0m                 if (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   3191\u001b[0m                 \u001b[0mscaled_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3192\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3193\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3195\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_accumulation_steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   2106\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2107\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2108\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2109\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2110\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m             )\n\u001b[0;32m--> 522\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    523\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    267\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "base_model_id = \"mistralai/Mistral-7B-v0.1\"\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_id,  # Mistral, same as before\n",
        "    quantization_config=bnb_config,  # Same quantization config as before\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "eval_tokenizer = AutoTokenizer.from_pretrained(base_model_id, add_bos_token=True, trust_remote_code=True)"
      ],
      "metadata": {
        "id": "2pHEKwMW4Y9K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import PeftModel\n",
        "\n",
        "ft_model = PeftModel.from_pretrained(base_model, \"output/checkpoint-300\")"
      ],
      "metadata": {
        "id": "lUMWQkXD4Z3N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_prompt = f\"### Title: {title}\\n ### Ingredients: {ingredients}\\n ### Instructions: {instructions}\"\n",
        "model_input = eval_tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "ft_model.eval()\n",
        "with torch.no_grad():\n",
        "    print(eval_tokenizer.decode(ft_model.generate(**model_input, max_new_tokens=100, repetition_penalty=1.15)[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "jYGtXxgN4q0k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), 'our_model/model_weights.pth')"
      ],
      "metadata": {
        "id": "YlY5H2hArH3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ingredients_list = [\"skinless\",\"onion\"]  # Sample ingredients you start with\n",
        "formatted_ingredients = \", \".join(ingredients_list)\n",
        "eval_prompt = f\"### Ingredients: {formatted_ingredients}\\n### Expected Output:\\n### Title: {{title}}\\n### Other Ingredients: {{other_ingredients}}\\n### Instructions: {{instructions}}\"\n",
        "\n",
        "odel_input = eval_tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "ft_model.eval()\n",
        "with torch.no_grad():\n",
        "    print(eval_tokenizer.decode(ft_model.generate(**model_input, max_new_tokens=100, repetition_penalty=1.15)[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "PXQlvv16tCaa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_prompt = f\"Ingredients: # \\n Expected Output:\\n Title: #\\n Other Ingredients: # \\n Instructions: #\"\n",
        "\n",
        "odel_input = eval_tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "ft_model.eval()\n",
        "with torch.no_grad():\n",
        "    print(eval_tokenizer.decode(ft_model.generate(**model_input, max_new_tokens=100, repetition_penalty=1.15)[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "ORW0E0Zbu0D8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}