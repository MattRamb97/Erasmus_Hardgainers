{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "908522ef9bd94e6f8a02e4642ee38a0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d8ce7a5f583f4fb7948614b68fd31404",
              "IPY_MODEL_6640c21d6b354e1fb37300e965bd062d",
              "IPY_MODEL_f24c5e848078403db2719e2c13c26f3d"
            ],
            "layout": "IPY_MODEL_58c64e0d7d764deb9a80ee6ab8a0be95",
            "tabbable": null,
            "tooltip": null
          }
        },
        "d8ce7a5f583f4fb7948614b68fd31404": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_80b02623217c4864a5802c1215d92f5a",
            "placeholder": "​",
            "style": "IPY_MODEL_a32c582e23e44efb86d9ab2203e7525f",
            "tabbable": null,
            "tooltip": null,
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "6640c21d6b354e1fb37300e965bd062d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_efdb470c4faf447e8fb73326da34a641",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8688af750cff4c9885fdf03c322b579b",
            "tabbable": null,
            "tooltip": null,
            "value": 2
          }
        },
        "f24c5e848078403db2719e2c13c26f3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_2d7614b2f7164f63bd78dc5de95ea1ff",
            "placeholder": "​",
            "style": "IPY_MODEL_23e56508698148a396955a35b8feb7da",
            "tabbable": null,
            "tooltip": null,
            "value": " 2/2 [00:08&lt;00:00,  3.98s/it]"
          }
        },
        "58c64e0d7d764deb9a80ee6ab8a0be95": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "80b02623217c4864a5802c1215d92f5a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a32c582e23e44efb86d9ab2203e7525f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLStyleModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "background": null,
            "description_width": "",
            "font_size": null,
            "text_color": null
          }
        },
        "efdb470c4faf447e8fb73326da34a641": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8688af750cff4c9885fdf03c322b579b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2d7614b2f7164f63bd78dc5de95ea1ff": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "23e56508698148a396955a35b8feb7da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLStyleModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "background": null,
            "description_width": "",
            "font_size": null,
            "text_color": null
          }
        },
        "ccc1bd2c0a454f96bd2fc895043dde0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6c7723b53aa749cf93987e4e5fa1da32",
              "IPY_MODEL_ce0066c99ea64879b679fb68afe2e47d",
              "IPY_MODEL_3fc14a395f1d4569865cbeea819c42c6"
            ],
            "layout": "IPY_MODEL_5f2a769a466e4c8ebed1b5eb555390cb",
            "tabbable": null,
            "tooltip": null
          }
        },
        "6c7723b53aa749cf93987e4e5fa1da32": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_9b238e7f08264805b524ac3100444476",
            "placeholder": "​",
            "style": "IPY_MODEL_7afe9062140a45099f85e96d07db9a27",
            "tabbable": null,
            "tooltip": null,
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "ce0066c99ea64879b679fb68afe2e47d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_ed6210d9752348ab9478e527c28c3c2b",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3c8da0d7d977480daec405cbd81ce475",
            "tabbable": null,
            "tooltip": null,
            "value": 2
          }
        },
        "3fc14a395f1d4569865cbeea819c42c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_f711cc2c2590496eb21e79a64e16660f",
            "placeholder": "​",
            "style": "IPY_MODEL_e3eb380a732c4ac4aa897968f546f1c8",
            "tabbable": null,
            "tooltip": null,
            "value": " 2/2 [00:56&lt;00:00, 26.61s/it]"
          }
        },
        "5f2a769a466e4c8ebed1b5eb555390cb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9b238e7f08264805b524ac3100444476": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7afe9062140a45099f85e96d07db9a27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLStyleModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "background": null,
            "description_width": "",
            "font_size": null,
            "text_color": null
          }
        },
        "ed6210d9752348ab9478e527c28c3c2b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3c8da0d7d977480daec405cbd81ce475": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f711cc2c2590496eb21e79a64e16660f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e3eb380a732c4ac4aa897968f546f1c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLStyleModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "background": null,
            "description_width": "",
            "font_size": null,
            "text_color": null
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MattRamb97/Erasmus_Hardgainers/blob/main/Recipes_model_v3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RECIPES MODEL**"
      ],
      "metadata": {
        "id": "QgSEjO0i7e_v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We utilized the URLs provided to access the datasets stored in the folder https://raw.githubusercontent.com/MattRamb97/Erasmus_Hardgainers/main/Datasets/\n",
        "\n",
        "using the pandas library. Subsequently, we identified an issue in the 'ingredients' column of the three datasets, characterized by the presence of the word 'ADVERTISEMENT', which we proceeded to remove. Additionally, we printed information regarding one of the datasets (the same information applies to the others) and their respective shapes."
      ],
      "metadata": {
        "id": "7EArwz7S8MEf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "zORKNY3QbWaP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be36e788-d367-48e9-a1c0-1a35fe2336de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 39802 entries, rmK12Uau.ntP510KeImX506H6Mr6jTu to 2Q3Zpfgt/PUwn1YABjJ5A9T3ZW8xwVa\n",
            "Data columns (total 4 columns):\n",
            " #   Column        Non-Null Count  Dtype \n",
            "---  ------        --------------  ----- \n",
            " 0   title         39522 non-null  object\n",
            " 1   ingredients   39522 non-null  object\n",
            " 2   instructions  39522 non-null  object\n",
            " 3   picture_link  39522 non-null  object\n",
            "dtypes: object(4)\n",
            "memory usage: 2.5+ MB\n",
            "\n",
            "From AR: (39522, 4)\n"
          ]
        }
      ],
      "source": [
        "url_1 = 'https://raw.githubusercontent.com/MattRamb97/Erasmus_Hardgainers/main/Datasets/recipes_raw_nosource_ar.json'\n",
        "\n",
        "import pandas as pd\n",
        "df_ar = pd.read_json(url_1, orient='records', dtype='dict').transpose()\n",
        "df_ar.info()\n",
        "print()\n",
        "\n",
        "df_ar = df_ar.dropna(subset=['title', 'ingredients', 'instructions', 'picture_link'])\n",
        "\n",
        "df_ar['title'] = df_ar['title'].astype(str)\n",
        "\n",
        "df_ar['ingredients'] = df_ar['ingredients'].astype(str).str.replace('ADVERTISEMENT', '',regex=True)\n",
        "\n",
        "# Safely evaluate the 'ingredients' column\n",
        "def safe_eval(x):\n",
        "    try:\n",
        "        return ', '.join(eval(x))\n",
        "    except:\n",
        "        return ''  # Return an empty string in case of an error\n",
        "\n",
        "df_ar['ingredients'] = df_ar['ingredients'].apply(safe_eval)\n",
        "\n",
        "df_ar['instructions'] = df_ar['instructions'].astype(str)\n",
        "\n",
        "print('From AR:', df_ar.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cleaning of the ingredients from numbers, units of measure and ( , ) , /"
      ],
      "metadata": {
        "id": "s6mJc7q2TC5S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def clean_ingredients(ingredient):\n",
        "    # Define the pattern to remove numbers, common units of measure, and unwanted characters including non-standard spaces\n",
        "    pattern = r'\\b\\d+\\.?\\d*|\\b(?:oz|ounce|ounces|lb|pound|pounds|g|gram|grams|kg|kilogram|kilograms|ml|milliliter|milliliters|l|liter|liters|tbsp|tbs|tablespoon|tablespoons|tsp|teaspoon|teaspoons|cup|cups|pinch|pinches)\\b|[()\\/]'\n",
        "    # Replace the matched items with nothing (effectively removing them)\n",
        "    cleaned_ingredient = re.sub(pattern, '', ingredient, flags=re.IGNORECASE)\n",
        "    # Remove any kind of space-like characters and trim the string\n",
        "    cleaned_ingredient = re.sub(r'\\s+', ' ', cleaned_ingredient).strip()\n",
        "    # Remove any invisible characters or non-breaking spaces\n",
        "    cleaned_ingredient = re.sub(r'[\\u200B-\\u200D\\uFEFF]', '', cleaned_ingredient)\n",
        "    return cleaned_ingredient\n",
        "\n",
        "# Apply the cleaning function to the ingredients column\n",
        "df_ar['ingredients'] = df_ar['ingredients'].apply(clean_ingredients)\n",
        "\n",
        "print(df_ar['ingredients'].iloc[2])"
      ],
      "metadata": {
        "id": "cZUHZoddMr1E",
        "outputId": "2098fc0d-2939-426f-e960-9ca9784dbd2f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "packed brown sugar , ketchup , lean ground beef , milk , eggs , salt , ground black pepper , small onion, chopped , ground ginger , finely crushed saltine cracker crumbs ,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We cleaned the three datasets using two function:\n",
        "- uncontract(): expands contracted words commonly found in English sentences into their full forms using regular expressions.\n",
        "- clean_recepies(): removes non-alphanumeric characters from the input text except for spaces and hyphens. It then replaces multiple spaces and hyphens with a single space and normalizes units of measurement (e.g., cups, tablespoons) by removing them. Next, it removes numerical digits, assuming they represent quantities. Finally, it normalizes whitespace by removing extra spaces and ensuring consistent spacing between words."
      ],
      "metadata": {
        "id": "3GcEDJTI-KL4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install accelerate\n",
        "!pip install -i https://pypi.org/simple/ bitsandbytes\n",
        "!pip install -q -U git+https://github.com/huggingface/transformers.git\n",
        "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
        "!pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
        "!pip install -q -U datasets scipy ipywidgets matplotlib"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fGLa2UZXF9g6",
        "outputId": "6521e95a-1fb4-4099-fb13-827cc35c4c3a"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (0.30.0.dev0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.2.1+cu121)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.22.2)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.13.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Looking in indexes: https://pypi.org/simple/\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (0.43.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.2.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.25.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.13.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->bitsandbytes) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->bitsandbytes) (1.3.0)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a formatting_func to structure training examples as prompts."
      ],
      "metadata": {
        "id": "MhNc7-SuHIUc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def formatting_func(ingredients, title, instructions):\n",
        "    text = f\"Ingredients: {ingredients} ### Expected Output: ### Title: {title} ### Instructions: {instructions}\"\n",
        "\n",
        "    return text"
      ],
      "metadata": {
        "id": "iP0ZOpuRGbb8"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Mistral - mistralai/Mistral-7B-v0.1"
      ],
      "metadata": {
        "id": "2dXRlDN-HM-U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "base_model_id = \"mistralai/Mistral-7B-v0.1\"\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(base_model_id, quantization_config=bnb_config, device_map=\"auto\")"
      ],
      "metadata": {
        "id": "I_MSkGl3UOiw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408,
          "referenced_widgets": [
            "908522ef9bd94e6f8a02e4642ee38a0c",
            "d8ce7a5f583f4fb7948614b68fd31404",
            "6640c21d6b354e1fb37300e965bd062d",
            "f24c5e848078403db2719e2c13c26f3d",
            "58c64e0d7d764deb9a80ee6ab8a0be95",
            "80b02623217c4864a5802c1215d92f5a",
            "a32c582e23e44efb86d9ab2203e7525f",
            "efdb470c4faf447e8fb73326da34a641",
            "8688af750cff4c9885fdf03c322b579b",
            "2d7614b2f7164f63bd78dc5de95ea1ff",
            "23e56508698148a396955a35b8feb7da"
          ]
        },
        "outputId": "279d1f3d-0889-443c-a6f7-5742023404a9"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    A token is already saved on your machine. Run `huggingface-cli whoami` to get more information or `huggingface-cli logout` if you want to log out.\n",
            "    Setting a new token will erase the existing one.\n",
            "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): \n",
            "Add token as git credential? (Y/n) Y\n",
            "Token is valid (permission: write).\n",
            "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
            "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
            "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
            "\n",
            "git config --global credential.helper store\n",
            "\n",
            "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
            "Token has not been saved to git credential helper.\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "908522ef9bd94e6f8a02e4642ee38a0c"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set up the tokenizer. Add padding on the left as it makes training use less memory. For model_max_length, it's helpful to get a distribution of your data lengths. Let's first tokenize without the truncation/padding, so we can get a length distribution."
      ],
      "metadata": {
        "id": "uh_eFIQ4IEYS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    base_model_id,\n",
        "    padding_side=\"left\",\n",
        "    add_eos_token=True,\n",
        "    add_bos_token=True,\n",
        ")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "def generate_and_tokenize_prompt(ingredients, title, instructions):\n",
        "    return tokenizer(formatting_func(ingredients, title, instructions))"
      ],
      "metadata": {
        "id": "SrALxLNtILcT"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reformat the prompt and tokenize each sample"
      ],
      "metadata": {
        "id": "6qxSioghIXcp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_titles = []\n",
        "tokenized_ingredients = []\n",
        "tokenized_instructions = []\n",
        "\n",
        "for ingredients, title, instructions in zip(df_ar['ingredients'].iloc[:1000], df_ar['title'].iloc[:1000], df_ar['instructions'].iloc[:1000]):\n",
        "    tokenized_ingredients.append(generate_and_tokenize_prompt( ingredients,title,instructions))\n",
        "    tokenized_titles.append(generate_and_tokenize_prompt( ingredients,title, instructions))\n",
        "    tokenized_instructions.append(generate_and_tokenize_prompt( ingredients,title, instructions))"
      ],
      "metadata": {
        "id": "Efy3OWxtIbNy"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's get a distribution of our dataset lengths, so we can determine the appropriate max_length for our input tensors."
      ],
      "metadata": {
        "id": "_1v-yvq2KZDG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_data_lengths(tokenized_titles, tokenized_ingredients):\n",
        "    lengths = [len(x['input_ids']) for x in tokenized_titles]\n",
        "    lengths += [len(x['input_ids']) for x in tokenized_ingredients]\n",
        "    print(len(lengths))\n",
        "\n",
        "    # Plotting the histogram\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.hist(lengths, bins=20, alpha=0.7, color='blue')\n",
        "    plt.xlabel('Length of input_ids')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.title('Distribution of Lengths of input_ids')\n",
        "    plt.show()\n",
        "\n",
        "plot_data_lengths(tokenized_titles, tokenized_ingredients)"
      ],
      "metadata": {
        "id": "sc9oNJ75KZwZ",
        "outputId": "297bd40e-a400-47df-dcd4-9c652120ccd6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        }
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2000\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 350  # This was an appropriate max length for my dataset\n",
        "\n",
        "def generate_and_tokenize_prompt2(ingredients, title, instructions):\n",
        "\n",
        "    ingredients_result = tokenizer(\n",
        "        ingredients,\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "    title_result = tokenizer(\n",
        "        title,\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "    instructions_result = tokenizer(\n",
        "        instructions,\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "    ingredients_result[\"labels\"] = ingredients_result[\"input_ids\"].copy()\n",
        "    title_result[\"labels\"] = title_result[\"input_ids\"].copy()\n",
        "    instructions_result[\"labels\"] = title_result[\"input_ids\"].copy()\n",
        "\n",
        "    return ingredients_result, title_result, instructions_result\n",
        "\n",
        "# Tokenization of the dataset\n",
        "tokenized_data = []\n",
        "for ingredients, title, instructions in zip(df_ar['ingredients'].iloc[:1000], df_ar['title'].iloc[:1000], df_ar['instructions'].iloc[:1000]):\n",
        "    ingredients_result, title_result, instructions_result = generate_and_tokenize_prompt2(ingredients, title, instructions)\n",
        "    # Concatenating results into a single input\n",
        "    combined_input_ids = ingredients_result[\"input_ids\"] + title_result[\"input_ids\"] + instructions_result[\"input_ids\"]\n",
        "    combined_labels = ingredients_result[\"labels\"] + title_result[\"labels\"] +  instructions_result[\"labels\"]\n",
        "    tokenized_data.append({'input_ids': combined_input_ids, 'labels': combined_labels})\n",
        "\n",
        "# Split data into training and evaluation datasets\n",
        "split_index = int(0.8 * len(tokenized_data))\n",
        "train_dataset = tokenized_data[:split_index]\n",
        "eval_dataset = tokenized_data[split_index:]"
      ],
      "metadata": {
        "id": "ZbTte5073fEL"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_prompt = f\"Ingredients: {ingredients} ### Expected Output: ### Title: {title} ### Instructions: {instructions}\"\n",
        "\n",
        "# Init an eval tokenizer that doesn't add padding or eos token\n",
        "eval_tokenizer = AutoTokenizer.from_pretrained(\n",
        "    base_model_id,\n",
        "    add_bos_token=True,\n",
        ")\n",
        "\n",
        "model_input = eval_tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    print(eval_tokenizer.decode(model.generate(**model_input, max_new_tokens=256, repetition_penalty=1.15)[0], skip_special_tokens=True))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fke7hj1JPyQT",
        "outputId": "5e5ca835-3aa1-4af6-9950-e7481f0c856b"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ingredients: all-purpose flour , salt , baking powder , white sugar , eggs , warm milk , butter, melted , vanilla extract , ### Expected Output: ### Title: Classic Waffles ### Instructions: In a large bowl, mix together flour, salt, baking powder and sugar; set aside. Preheat waffle iron to desired temperature.\n",
            "In a separate bowl, beat the eggs. Stir in the milk, butter and vanilla. Pour the milk mixture into the flour mixture; beat until blended.\n",
            "Ladle the batter into a preheated waffle iron. Cook the waffles until golden and crisp. Serve immediately.\n",
            "### Notes:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "k4qY_S0EaOPs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Set Up LoRA**"
      ],
      "metadata": {
        "id": "JMDv9mCeV-il"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, to start our fine-tuning, we have to apply some preprocessing to the model to prepare it for training. For that use the prepare_model_for_kbit_training method from PEFT."
      ],
      "metadata": {
        "id": "txXs94_zWCcQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import prepare_model_for_kbit_training\n",
        "\n",
        "model.gradient_checkpointing_enable()\n",
        "model = prepare_model_for_kbit_training(model)"
      ],
      "metadata": {
        "id": "k-rUo6KTWFMU"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_trainable_parameters(model):\n",
        "    \"\"\"\n",
        "    Prints the number of trainable parameters in the model.\n",
        "    \"\"\"\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    print(\n",
        "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
        "    )"
      ],
      "metadata": {
        "id": "JTws6uDbWIOD"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's print the model to examine its layers, as we will apply QLoRA to all the linear layers of the model. Those layers are q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, down_proj, and lm_head."
      ],
      "metadata": {
        "id": "YMHP0xk-WNVV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "id": "z-6Iqd0KYCID",
        "outputId": "a0569bd1-2358-4f92-c5f8-336286a831be",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PeftModelForCausalLM(\n",
            "  (base_model): LoraModel(\n",
            "    (model): MistralForCausalLM(\n",
            "      (model): MistralModel(\n",
            "        (embed_tokens): Embedding(32000, 4096)\n",
            "        (layers): ModuleList(\n",
            "          (0-31): 32 x MistralDecoderLayer(\n",
            "            (self_attn): MistralSdpaAttention(\n",
            "              (q_proj): lora.Linear4bit(\n",
            "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "              )\n",
            "              (k_proj): lora.Linear4bit(\n",
            "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "              )\n",
            "              (v_proj): lora.Linear4bit(\n",
            "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=32, out_features=1024, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "              )\n",
            "              (o_proj): lora.Linear4bit(\n",
            "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "              )\n",
            "              (rotary_emb): MistralRotaryEmbedding()\n",
            "            )\n",
            "            (mlp): MistralMLP(\n",
            "              (gate_proj): lora.Linear4bit(\n",
            "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=32, out_features=14336, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "              )\n",
            "              (up_proj): lora.Linear4bit(\n",
            "                (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=4096, out_features=32, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=32, out_features=14336, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "              )\n",
            "              (down_proj): lora.Linear4bit(\n",
            "                (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
            "                (lora_dropout): ModuleDict(\n",
            "                  (default): Dropout(p=0.05, inplace=False)\n",
            "                )\n",
            "                (lora_A): ModuleDict(\n",
            "                  (default): Linear(in_features=14336, out_features=32, bias=False)\n",
            "                )\n",
            "                (lora_B): ModuleDict(\n",
            "                  (default): Linear(in_features=32, out_features=4096, bias=False)\n",
            "                )\n",
            "                (lora_embedding_A): ParameterDict()\n",
            "                (lora_embedding_B): ParameterDict()\n",
            "              )\n",
            "              (act_fn): SiLU()\n",
            "            )\n",
            "            (input_layernorm): MistralRMSNorm()\n",
            "            (post_attention_layernorm): MistralRMSNorm()\n",
            "          )\n",
            "        )\n",
            "        (norm): MistralRMSNorm()\n",
            "      )\n",
            "      (lm_head): lora.Linear(\n",
            "        (base_layer): Linear(in_features=4096, out_features=32000, bias=False)\n",
            "        (lora_dropout): ModuleDict(\n",
            "          (default): Dropout(p=0.05, inplace=False)\n",
            "        )\n",
            "        (lora_A): ModuleDict(\n",
            "          (default): Linear(in_features=4096, out_features=32, bias=False)\n",
            "        )\n",
            "        (lora_B): ModuleDict(\n",
            "          (default): Linear(in_features=32, out_features=32000, bias=False)\n",
            "        )\n",
            "        (lora_embedding_A): ParameterDict()\n",
            "        (lora_embedding_B): ParameterDict()\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we define the LoRA config.\n",
        "\n",
        "r is the rank of the low-rank matrix used in the adapters, which thus controls the number of parameters trained. A higher rank will allow for more expressivity, but there is a compute tradeoff.\n",
        "\n",
        "alpha is the scaling factor for the learned weights. The weight matrix is scaled by alpha/r, and thus a higher value for alpha assigns more weight to the LoRA activations.\n",
        "\n",
        "The values used in the QLoRA paper were r=64 and lora_alpha=16, and these are said to generalize well, but we will use r=32 and lora_alpha=64 so that we have more emphasis on the new fine-tuned data while also reducing computational complexity."
      ],
      "metadata": {
        "id": "d13VfHnQYHw5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "config = LoraConfig(\n",
        "    r=32,\n",
        "    lora_alpha=64,\n",
        "    target_modules=[\n",
        "        \"q_proj\",\n",
        "        \"k_proj\",\n",
        "        \"v_proj\",\n",
        "        \"o_proj\",\n",
        "        \"gate_proj\",\n",
        "        \"up_proj\",\n",
        "        \"down_proj\",\n",
        "        \"lm_head\",\n",
        "    ],\n",
        "    bias=\"none\",\n",
        "    lora_dropout=0.05,  # Conventional\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, config)\n",
        "print_trainable_parameters(model)"
      ],
      "metadata": {
        "id": "yvugWrLvYPZU",
        "outputId": "ac8a27cb-5ca3-4d55-ba69-7a1418a6f90a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 85041152 || all params: 3837112320 || trainable%: 2.2162799758751914\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "See how the model looks different now, with the LoRA adapters added:"
      ],
      "metadata": {
        "id": "4xhf8d0LYUdD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "id": "v6y9Rys9YWey",
        "outputId": "95594d1f-6562-4cb6-a152-8da3f24cf272",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PeftModelForCausalLM(\n",
            "  (base_model): LoraModel(\n",
            "    (model): PeftModelForCausalLM(\n",
            "      (base_model): LoraModel(\n",
            "        (model): MistralForCausalLM(\n",
            "          (model): MistralModel(\n",
            "            (embed_tokens): Embedding(32000, 4096)\n",
            "            (layers): ModuleList(\n",
            "              (0-31): 32 x MistralDecoderLayer(\n",
            "                (self_attn): MistralSdpaAttention(\n",
            "                  (q_proj): lora.Linear4bit(\n",
            "                    (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
            "                    (lora_dropout): ModuleDict(\n",
            "                      (default): Dropout(p=0.05, inplace=False)\n",
            "                    )\n",
            "                    (lora_A): ModuleDict(\n",
            "                      (default): Linear(in_features=4096, out_features=32, bias=False)\n",
            "                    )\n",
            "                    (lora_B): ModuleDict(\n",
            "                      (default): Linear(in_features=32, out_features=4096, bias=False)\n",
            "                    )\n",
            "                    (lora_embedding_A): ParameterDict()\n",
            "                    (lora_embedding_B): ParameterDict()\n",
            "                  )\n",
            "                  (k_proj): lora.Linear4bit(\n",
            "                    (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
            "                    (lora_dropout): ModuleDict(\n",
            "                      (default): Dropout(p=0.05, inplace=False)\n",
            "                    )\n",
            "                    (lora_A): ModuleDict(\n",
            "                      (default): Linear(in_features=4096, out_features=32, bias=False)\n",
            "                    )\n",
            "                    (lora_B): ModuleDict(\n",
            "                      (default): Linear(in_features=32, out_features=1024, bias=False)\n",
            "                    )\n",
            "                    (lora_embedding_A): ParameterDict()\n",
            "                    (lora_embedding_B): ParameterDict()\n",
            "                  )\n",
            "                  (v_proj): lora.Linear4bit(\n",
            "                    (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
            "                    (lora_dropout): ModuleDict(\n",
            "                      (default): Dropout(p=0.05, inplace=False)\n",
            "                    )\n",
            "                    (lora_A): ModuleDict(\n",
            "                      (default): Linear(in_features=4096, out_features=32, bias=False)\n",
            "                    )\n",
            "                    (lora_B): ModuleDict(\n",
            "                      (default): Linear(in_features=32, out_features=1024, bias=False)\n",
            "                    )\n",
            "                    (lora_embedding_A): ParameterDict()\n",
            "                    (lora_embedding_B): ParameterDict()\n",
            "                  )\n",
            "                  (o_proj): lora.Linear4bit(\n",
            "                    (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
            "                    (lora_dropout): ModuleDict(\n",
            "                      (default): Dropout(p=0.05, inplace=False)\n",
            "                    )\n",
            "                    (lora_A): ModuleDict(\n",
            "                      (default): Linear(in_features=4096, out_features=32, bias=False)\n",
            "                    )\n",
            "                    (lora_B): ModuleDict(\n",
            "                      (default): Linear(in_features=32, out_features=4096, bias=False)\n",
            "                    )\n",
            "                    (lora_embedding_A): ParameterDict()\n",
            "                    (lora_embedding_B): ParameterDict()\n",
            "                  )\n",
            "                  (rotary_emb): MistralRotaryEmbedding()\n",
            "                )\n",
            "                (mlp): MistralMLP(\n",
            "                  (gate_proj): lora.Linear4bit(\n",
            "                    (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
            "                    (lora_dropout): ModuleDict(\n",
            "                      (default): Dropout(p=0.05, inplace=False)\n",
            "                    )\n",
            "                    (lora_A): ModuleDict(\n",
            "                      (default): Linear(in_features=4096, out_features=32, bias=False)\n",
            "                    )\n",
            "                    (lora_B): ModuleDict(\n",
            "                      (default): Linear(in_features=32, out_features=14336, bias=False)\n",
            "                    )\n",
            "                    (lora_embedding_A): ParameterDict()\n",
            "                    (lora_embedding_B): ParameterDict()\n",
            "                  )\n",
            "                  (up_proj): lora.Linear4bit(\n",
            "                    (base_layer): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
            "                    (lora_dropout): ModuleDict(\n",
            "                      (default): Dropout(p=0.05, inplace=False)\n",
            "                    )\n",
            "                    (lora_A): ModuleDict(\n",
            "                      (default): Linear(in_features=4096, out_features=32, bias=False)\n",
            "                    )\n",
            "                    (lora_B): ModuleDict(\n",
            "                      (default): Linear(in_features=32, out_features=14336, bias=False)\n",
            "                    )\n",
            "                    (lora_embedding_A): ParameterDict()\n",
            "                    (lora_embedding_B): ParameterDict()\n",
            "                  )\n",
            "                  (down_proj): lora.Linear4bit(\n",
            "                    (base_layer): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
            "                    (lora_dropout): ModuleDict(\n",
            "                      (default): Dropout(p=0.05, inplace=False)\n",
            "                    )\n",
            "                    (lora_A): ModuleDict(\n",
            "                      (default): Linear(in_features=14336, out_features=32, bias=False)\n",
            "                    )\n",
            "                    (lora_B): ModuleDict(\n",
            "                      (default): Linear(in_features=32, out_features=4096, bias=False)\n",
            "                    )\n",
            "                    (lora_embedding_A): ParameterDict()\n",
            "                    (lora_embedding_B): ParameterDict()\n",
            "                  )\n",
            "                  (act_fn): SiLU()\n",
            "                )\n",
            "                (input_layernorm): MistralRMSNorm()\n",
            "                (post_attention_layernorm): MistralRMSNorm()\n",
            "              )\n",
            "            )\n",
            "            (norm): MistralRMSNorm()\n",
            "          )\n",
            "          (lm_head): lora.Linear(\n",
            "            (base_layer): Linear(in_features=4096, out_features=32000, bias=False)\n",
            "            (lora_dropout): ModuleDict(\n",
            "              (default): Dropout(p=0.05, inplace=False)\n",
            "            )\n",
            "            (lora_A): ModuleDict(\n",
            "              (default): Linear(in_features=4096, out_features=32, bias=False)\n",
            "            )\n",
            "            (lora_B): ModuleDict(\n",
            "              (default): Linear(in_features=32, out_features=32000, bias=False)\n",
            "            )\n",
            "            (lora_embedding_A): ParameterDict()\n",
            "            (lora_embedding_B): ParameterDict()\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "tIu5yNhYaRmq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TRAINING LoRA MODEL**"
      ],
      "metadata": {
        "id": "nkBAVmb3ZZ6e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.device_count() > 1: # If more than 1 GPU\n",
        "    model.is_parallelizable = True\n",
        "    model.model_parallel = True"
      ],
      "metadata": {
        "id": "PGTxRTgLZi_0"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from accelerate import FullyShardedDataParallelPlugin, Accelerator\n",
        "from torch.distributed.fsdp.fully_sharded_data_parallel import FullOptimStateDictConfig, FullStateDictConfig\n",
        "\n",
        "fsdp_plugin = FullyShardedDataParallelPlugin(\n",
        "    state_dict_config=FullStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
        "    optim_state_dict_config=FullOptimStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
        ")\n",
        "\n",
        "accelerator = Accelerator(fsdp_plugin=fsdp_plugin)\n",
        "\n",
        "model = accelerator.prepare_model(model)"
      ],
      "metadata": {
        "id": "J4Nh-UgTZktM"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q wandb -U\n",
        "\n",
        "import wandb, os\n",
        "wandb.login()\n",
        "\n",
        "wandb_project = \"journal-finetune\"\n",
        "if len(wandb_project) > 0:\n",
        "    os.environ[\"WANDB_PROJECT\"] = wandb_project"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_QY-O7nOa3AZ",
        "outputId": "f4ace141-0cdd-431c-dabf-b9a490e52185"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "from transformers import TrainingArguments, Trainer\n",
        "from datetime import datetime\n",
        "import torch\n",
        "\n",
        "project = \"recepies-model\"\n",
        "base_model_name = \"mistral\"\n",
        "run_name = base_model_name + \"-\" + project\n",
        "output_dir = \"./\" + run_name\n",
        "\n",
        "# Initialize Trainer with your model and tokenizer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=TrainingArguments(\n",
        "        output_dir='./output',\n",
        "        warmup_steps=1,\n",
        "        per_device_train_batch_size=2,\n",
        "        gradient_accumulation_steps=1,\n",
        "        gradient_checkpointing=True,\n",
        "        max_steps=500,\n",
        "        learning_rate=2.5e-5,\n",
        "        fp16=True,\n",
        "        optim=\"paged_adamw_8bit\",\n",
        "        logging_steps=25,\n",
        "        logging_dir=\"./logs\",\n",
        "        save_strategy=\"steps\",\n",
        "        save_steps=25,\n",
        "        evaluation_strategy=\"steps\",\n",
        "        eval_steps=25,\n",
        "        do_eval=True,\n",
        "        report_to=\"wandb\",\n",
        "        run_name=f\"{run_name}-{datetime.now().strftime('%Y-%m-%d-%H-%M')}\"\n",
        "    ),\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
        ")\n",
        "model.config.use_cache = False\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 974
        },
        "id": "BeEHSdGmZyjQ",
        "outputId": "12c8bf03-2316-4a2a-c40d-96655b66fe2d"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1463: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "max_steps is given, it will override any value given in num_train_epochs\n",
            "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:180: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
            "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='133' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [133/500 22:10 < 1:02:06, 0.10 it/s, Epoch 0.33/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>0.900900</td>\n",
              "      <td>0.926720</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.837200</td>\n",
              "      <td>0.906595</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>75</td>\n",
              "      <td>0.793200</td>\n",
              "      <td>0.893913</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.801300</td>\n",
              "      <td>0.880075</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>125</td>\n",
              "      <td>0.821600</td>\n",
              "      <td>0.879000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:180: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
            "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:180: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
            "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:180: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
            "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:180: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n",
            "  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-82-090d3ef20387>\u001b[0m in \u001b[0;36m<cell line: 39>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m )\n\u001b[1;32m     38\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_cache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1873\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1874\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1875\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   1876\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1877\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2220\u001b[0m                     \u001b[0mtr_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtr_loss_step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2222\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_flos\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloating_point_ops\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2224\u001b[0m                 is_last_step_and_steps_less_than_grad_acc = (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mfloating_point_ops\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3924\u001b[0m         \"\"\"\n\u001b[1;32m   3925\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"floating_point_ops\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3926\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloating_point_ops\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3927\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3928\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfloating_point_ops\u001b[0;34m(self, input_dict, exclude_embeddings)\u001b[0m\n\u001b[1;32m   1218\u001b[0m         \"\"\"\n\u001b[1;32m   1219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1220\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;36m6\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimate_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexclude_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexclude_embeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mnum_parameters\u001b[0;34m(self, only_trainable, exclude_embeddings)\u001b[0m\n\u001b[1;32m   1138\u001b[0m                 \u001b[0;34mf\"{name}.weight\"\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_modules\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m             ]\n\u001b[0;32m-> 1140\u001b[0;31m             total_parameters = [\n\u001b[0m\u001b[1;32m   1141\u001b[0m                 \u001b[0mparameter\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0membedding_param_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m             ]\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1138\u001b[0m                 \u001b[0;34mf\"{name}.weight\"\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_modules\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m             ]\n\u001b[0;32m-> 1140\u001b[0;31m             total_parameters = [\n\u001b[0m\u001b[1;32m   1141\u001b[0m                 \u001b[0mparameter\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0membedding_param_names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m             ]\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mnamed_parameters\u001b[0;34m(self, prefix, recurse, remove_duplicate)\u001b[0m\n\u001b[1;32m   2224\u001b[0m             \u001b[0;32mlambda\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2225\u001b[0m             prefix=prefix, recurse=recurse, remove_duplicate=remove_duplicate)\n\u001b[0;32m-> 2226\u001b[0;31m         \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2228\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mbuffers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mIterator\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_named_members\u001b[0;34m(self, get_members_fn, prefix, recurse, remove_duplicate)\u001b[0m\n\u001b[1;32m   2159\u001b[0m         \u001b[0mmemo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2160\u001b[0m         \u001b[0mmodules\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_modules\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremove_duplicate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mremove_duplicate\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2161\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule_prefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2162\u001b[0m             \u001b[0mmembers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_members_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2163\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmembers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mnamed_modules\u001b[0;34m(self, memo, prefix, remove_duplicate)\u001b[0m\n\u001b[1;32m   2371\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2372\u001b[0m                 \u001b[0msubmodule_prefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mprefix\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2373\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_modules\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubmodule_prefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremove_duplicate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2375\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mnamed_modules\u001b[0;34m(self, memo, prefix, remove_duplicate)\u001b[0m\n\u001b[1;32m   2371\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2372\u001b[0m                 \u001b[0msubmodule_prefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mprefix\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2373\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_modules\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubmodule_prefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremove_duplicate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2375\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mnamed_modules\u001b[0;34m(self, memo, prefix, remove_duplicate)\u001b[0m\n\u001b[1;32m   2371\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2372\u001b[0m                 \u001b[0msubmodule_prefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mprefix\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2373\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_modules\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubmodule_prefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremove_duplicate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2375\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mnamed_modules\u001b[0;34m(self, memo, prefix, remove_duplicate)\u001b[0m\n\u001b[1;32m   2371\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2372\u001b[0m                 \u001b[0msubmodule_prefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mprefix\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2373\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_modules\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubmodule_prefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremove_duplicate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2375\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mnamed_modules\u001b[0;34m(self, memo, prefix, remove_duplicate)\u001b[0m\n\u001b[1;32m   2368\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2369\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2370\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2371\u001b[0m                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2372\u001b[0m                 \u001b[0msubmodule_prefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mprefix\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "base_model_id = \"mistralai/Mistral-7B-v0.1\"\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_id,  # Mistral, same as before\n",
        "    quantization_config=bnb_config,  # Same quantization config as before\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "eval_tokenizer = AutoTokenizer.from_pretrained(base_model_id, add_bos_token=True, trust_remote_code=True)"
      ],
      "metadata": {
        "id": "2pHEKwMW4Y9K",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "ccc1bd2c0a454f96bd2fc895043dde0f",
            "6c7723b53aa749cf93987e4e5fa1da32",
            "ce0066c99ea64879b679fb68afe2e47d",
            "3fc14a395f1d4569865cbeea819c42c6",
            "5f2a769a466e4c8ebed1b5eb555390cb",
            "9b238e7f08264805b524ac3100444476",
            "7afe9062140a45099f85e96d07db9a27",
            "ed6210d9752348ab9478e527c28c3c2b",
            "3c8da0d7d977480daec405cbd81ce475",
            "f711cc2c2590496eb21e79a64e16660f",
            "e3eb380a732c4ac4aa897968f546f1c8"
          ]
        },
        "outputId": "3cdba9a4-bb89-4043-b33b-f99d7aeadcaf"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ccc1bd2c0a454f96bd2fc895043dde0f"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import PeftModel\n",
        "\n",
        "ft_model = PeftModel.from_pretrained(base_model, \"output/checkpoint-300\")"
      ],
      "metadata": {
        "id": "lUMWQkXD4Z3N"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_prompt = f\"Ingredients: {df_ar['ingredients'].iloc[1]} ### Expected Output: ### Title: {title} ### Instructions: {instructions}\"\n",
        "\n",
        "#f\"### Ingredients: {df_ar['ingredients'].iloc[1]}\\n### Generate Title and Instructions:\\n### Title:\"\n",
        "\n",
        "model_input = eval_tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "ft_model.eval()\n",
        "with torch.no_grad():\n",
        "    generated_texts = eval_tokenizer.decode(ft_model.generate(**model_input, max_new_tokens=100, repetition_penalty=1.15)[0], skip_special_tokens=True)\n",
        "    print(generated_texts)"
      ],
      "metadata": {
        "id": "mwm1UxK68tdE",
        "outputId": "812b5fb1-5bee-4c50-bad1-058b55fd7164",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### Ingredients: \n",
            "### Generate Title and Instructions:\n",
            "### Title:\n",
            "### Instructions:\n",
            "### Prep Time:\n",
            "### Cook Time:\n",
            "### Yield:\n",
            "### Nutrition Facts:\n",
            "### Serving Size:\n",
            "### Calories:\n",
            "### Fat:\n",
            "### Saturated Fat:\n",
            "### Cholesterol:\n",
            "### Sodium:\n",
            "### Carbohydrates:\n",
            "### Fiber:\n",
            "### Sugar:\n",
            "### Protein:\n",
            "### Ingredients:\n",
            "### Directions:\n",
            "\n",
            "import\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def evaluate_and_plot(generated_text, reference_text):\n",
        "    from datasets import load_metric\n",
        "    bleu_metric = load_metric('bleu')\n",
        "    rouge_metric = load_metric('rouge')\n",
        "    meteor_metric = load_metric('meteor')\n",
        "\n",
        "    predictions = [generated_text.split()]\n",
        "    references = [[reference_text.split()]]\n",
        "\n",
        "    # Compute metrics\n",
        "    bleu_result = bleu_metric.compute(predictions=predictions, references=references)['bleu']\n",
        "    rouge_result = rouge_metric.compute(predictions=[generated_text], references=[reference_text])['rouge-1']['f']\n",
        "    meteor_result = meteor_metric.compute(predictions=[generated_text], references=[reference_text])['meteor']\n",
        "\n",
        "    # Prepare data for plotting\n",
        "    metrics = ['BLEU', 'ROUGE-1', 'METEOR']\n",
        "    scores = [bleu_result, rouge_result, meteor_result]\n",
        "\n",
        "    # Plotting\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    plt.bar(metrics, scores, color=['blue', 'green', 'red'])\n",
        "    plt.xlabel('Metrics')\n",
        "    plt.ylabel('Scores')\n",
        "    plt.title('Evaluation Metrics')\n",
        "    plt.ylim(0, 1)  # Assuming the scores are normalized between 0 and 1\n",
        "    plt.show()\n",
        "\n",
        "# Example generated and reference texts\n",
        "reference_texts = f\"Ingredients: {df_ar['ingredients'].iloc[1]} ### Expected Output: ### Title: {df_ar['title'].iloc[1]} ### Instructions: {df_ar['instructions'].iloc[1]}\"\n",
        "#f\"### Ingredients: {df_ar['ingredients'].iloc[1]}\\n### Generate Title and Instructions:\\n### Title:{df_ar['title'].iloc[1]}\\nInstrictions:{df_ar['instructions'].iloc[1]}\"\n",
        "\n",
        "\n",
        "# Generate plot\n",
        "evaluate_and_plot(generated_texts, reference_texts)"
      ],
      "metadata": {
        "id": "obaG_MZzuOvr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}